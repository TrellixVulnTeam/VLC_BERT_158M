# VLC-BERT (11747-project)

## Authors 
Shentong Mo,  Jingfei Xia,  Ihor Markevych

## Abstract
Visual and linguistic pre-training aims to learn vision and language representations together, which can be transferred to visual-linguistic downstream tasks. In this work, we first explore three previous models for pre-training visual and language representations together. Then we evaluate those pre-trained models on two main downstream tasks, including Visual Question Answering (VQA) and Visual Commonsense Reasoning (VCR). Our goal is to propose a novel network based on BERT for pre-training visual and linguistic representations contrastively, namely **VLC-BERT**, which applies a contrastive learning framework during the pre-training stage. Compared with three baselines, our VLC-BERT pre-trained models are expected to achieve comparable or better results when transferred to VQA and VCR tasks. 

## Baselines

Please see the details about those baselines in each branch.
--ViLBERT 
--VisualBERT
--VL-BERT


## References

